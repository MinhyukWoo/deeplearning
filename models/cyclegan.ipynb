{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL='https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip'\n",
    "# ZIP_FILE='../dataset/horse2zebra.zip'\n",
    "# TARGET_DIR='../dataset/horse2zebra'\n",
    "# !mkdir -p ../dataset\n",
    "# !wget -N $URL -O $ZIP_FILE\n",
    "# !unzip $ZIP_FILE -d ../dataset/\n",
    "# !rm $ZIP_FILE\n",
    "\n",
    "# # Adapt to project expected directory heriarchy\n",
    "# !mkdir -p \"$TARGET_DIR/train\" \"$TARGET_DIR/test\"\n",
    "# !mv \"$TARGET_DIR/trainA\" \"$TARGET_DIR/train/A\"\n",
    "# !mv \"$TARGET_DIR/trainB\" \"$TARGET_DIR/train/B\"\n",
    "# !mv \"$TARGET_DIR/testA\" \"$TARGET_DIR/test/A\"\n",
    "# !mv \"$TARGET_DIR/testB\" \"$TARGET_DIR/test/B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=0, n_epochs=200, batchSize=1, dataroot='../dataset/horse2zebra/', lr=0.0002, decay_epoch=100, size=256, input_nc=3, output_nc=3, cuda=False, n_cpu=8)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
    "parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "parser.add_argument('--batchSize', type=int, default=1, help='size of the batches')\n",
    "parser.add_argument('--dataroot', type=str, default='../dataset/horse2zebra/', help='root directory of the dataset')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='epoch to start linearly decaying the learning rate to 0')\n",
    "parser.add_argument('--size', type=int, default=256, help='size of the data crop (squared assumed)')\n",
    "parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')\n",
    "parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')\n",
    "parser.add_argument('--cuda', action='store_true', help='use GPU computation')\n",
    "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "opt = parser.parse_args('')\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToTensor()(Image.open(os.path.join(TARGET_DIR, 'train', 'A', 'n02381460_2.jpg'))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%s/A' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%s/B' % mode) + '/*.*'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        return {'A': item_A, 'B': item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel) -> None:\n",
    "        super(GenConvolutionBlock, self).__init__()\n",
    "        padding = kernel // 2\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, stride=1, padding=padding, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_last_layer=False) -> None:\n",
    "        super(DisConvolutionBlock, self).__init__()\n",
    "        out_channels = 1 if is_last_layer else out_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            *[nn.Sigmoid()]\n",
    "            if is_last_layer\n",
    "            else [nn.InstanceNorm2d(out_channels), nn.LeakyReLU(0.02, inplace=True)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect'),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): GenConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DownsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DownsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (6): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (7): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (8): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (9): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (12): UpsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (13): UpsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (14): GenConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "generator_layers_config = [\n",
    "    \"c7s1-64\",\n",
    "    \"d128\",\n",
    "    \"d256\",\n",
    "    *(\"R256\" for _ in range(9)),\n",
    "    \"u128\",\n",
    "    \"u64\",\n",
    "    \"c7s1-3\",\n",
    "]\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, layers_config=generator_layers_config):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Generator Model\n",
    "        model = []\n",
    "        in_channels = input_nc\n",
    "        for layer_config in layers_config:\n",
    "            type = layer_config[0:1]\n",
    "            if type == \"c\":\n",
    "                kernel_size, _, out_channels = (\n",
    "                    int(outs)\n",
    "                    for outs in re.match(\"c(.+)s(.+)-(.+)\", layer_config).groups()\n",
    "                )\n",
    "                model.append(GenConvolutionBlock(in_channels, out_channels, kernel_size))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"d\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(DownsamplingBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"R\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"u\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(UpsamplingBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "generator = Generator(3, 3)\n",
    "print(generator)\n",
    "generator(torch.zeros((1, 3, 256, 256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "discriminator_layers_config = [\n",
    "    \"C64\",\n",
    "    \"C128\",\n",
    "    \"C256\",\n",
    "    \"C512\",\n",
    "]\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, layers_config=discriminator_layers_config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Discriminator Model\n",
    "        model = []\n",
    "        in_channels = input_nc\n",
    "        for layer_config in layers_config:\n",
    "            type = layer_config[0:1]\n",
    "            if type == \"C\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(DisConvolutionBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "        model.append(DisConvolutionBlock(in_channels, 1, True))\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "\n",
    "discriminator = Discriminator(3)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lossess\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
    "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
    "netD_A = Discriminator(opt.input_nc)\n",
    "netD_B = Discriminator(opt.output_nc)\n",
    "\n",
    "if opt.cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "# netG_A2B.apply(weights_init_normal)\n",
    "# netG_B2A.apply(weights_init_normal)\n",
    "# netD_A.apply(weights_init_normal)\n",
    "# netD_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
    "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1203/3425252401.py:2: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms_ = [ transforms.Resize(int(opt.size*1.12), Image.BICUBIC),\n",
      "/home/minhyuk/miniconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dataset loader\n",
    "transforms_ = [ transforms.Resize(int(opt.size*1.12), Image.BICUBIC), \n",
    "                transforms.RandomCrop(opt.size), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True), \n",
    "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/200 batch:1/1334\n",
      "  pred_real_A:0.7690 | pred_fake_A:0.2839\n",
      "  pred_real_B:0.7181 | pred_fake_B:0.3752\n",
      "  loss_G:17.8905 | loss_D_A:0.0670 | loss_D_B:0.1101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb ì…€ 20\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Total loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m loss_G \u001b[39m=\u001b[39m loss_identity_A \u001b[39m+\u001b[39m loss_identity_B \u001b[39m+\u001b[39m loss_GAN_A2B \u001b[39m+\u001b[39m loss_GAN_B2A \u001b[39m+\u001b[39m loss_cycle_ABA \u001b[39m+\u001b[39m loss_cycle_BAB\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m loss_G\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer_G\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m###################################\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/deeplearning/models/cyclegan.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m###### Discriminator A ######\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss (lambda=5)\n",
    "        # G_A2B(B) should equal B if real B is fed     \n",
    "        loss_identity_A = criterion_identity(netG_B2A(real_A), real_A) * 5.0\n",
    "\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        loss_identity_B = criterion_identity(netG_A2B(real_B), real_B) * 5.0\n",
    "\n",
    "        # GAN loss (lambda=1)\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_A = netD_A(fake_A)\n",
    "        pred_B = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_B, target_real)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_A, target_real)\n",
    "\n",
    "        # Cycle loss (lambda=10)\n",
    "        fake_ABA = netG_B2A(fake_B)\n",
    "        fake_BAB = netG_A2B(fake_A)\n",
    "        loss_cycle_ABA = criterion_cycle(fake_ABA, real_A) * 10.0\n",
    "        loss_cycle_BAB = criterion_cycle(fake_BAB, real_B) * 10.0\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_A = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real_A, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake_A = netD_A(fake_A)\n",
    "        loss_D_fake = criterion_GAN(pred_fake_A, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_B = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real_B, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake_B = netD_B(fake_B)\n",
    "        loss_D_fake = criterion_GAN(pred_fake_B, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "        if i % 330 == 0:\n",
    "            print(f'epoch:{epoch+1}/{opt.n_epochs} batch:{i+1}/{len(dataloader)}')\n",
    "            print(f'  pred_real_A:{pred_real_A.mean():.4f} | pred_fake_A:{pred_fake_A.mean():.4f}')\n",
    "            print(f'  pred_real_B:{pred_real_B.mean():.4f} | pred_fake_B:{pred_fake_B.mean():.4f}')\n",
    "            print(f'  loss_G:{loss_G:.4f} | loss_D_A:{loss_D_A:.4f} | loss_D_B:{loss_D_B:.4f}')\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Testing######\n",
    "\n",
    "# Set model's test mode\n",
    "netG_A2B.eval()\n",
    "netG_B2A.eval()\n",
    "\n",
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [ transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, mode='test'), \n",
    "                        batch_size=opt.batchSize, shuffle=False, num_workers=opt.n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output dirs if they don't exist\n",
    "if not os.path.exists('output/A'):\n",
    "    os.makedirs('output/A')\n",
    "if not os.path.exists('output/B'):\n",
    "    os.makedirs('output/B')\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # Set model input\n",
    "    real_A = Variable(input_A.copy_(batch['A']))\n",
    "    real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "    # Generate output\n",
    "    fake_B = 0.5*(netG_A2B(real_A).data + 1.0)\n",
    "    fake_A = 0.5*(netG_B2A(real_B).data + 1.0)\n",
    "\n",
    "    # Save image files\n",
    "    save_image(fake_A, 'output/A/%04d.png' % (i+1))\n",
    "    save_image(fake_B, 'output/B/%04d.png' % (i+1))\n",
    "\n",
    "    sys.stdout.write('\\rGenerated images %04d of %04d' % (i+1, len(dataloader)))\n",
    "\n",
    "sys.stdout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

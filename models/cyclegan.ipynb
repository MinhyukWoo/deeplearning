{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL='https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip'\n",
    "ZIP_FILE='../dataset/horse2zebra.zip'\n",
    "TARGET_DIR='../dataset/horse2zebra'\n",
    "# !mkdir -p ../dataset\n",
    "# !wget -N $URL -O $ZIP_FILE\n",
    "# !unzip $ZIP_FILE -d ../dataset/\n",
    "# !rm $ZIP_FILE\n",
    "\n",
    "# # Adapt to project expected directory heriarchy\n",
    "# !mkdir -p \"$TARGET_DIR/train\" \"$TARGET_DIR/test\"\n",
    "# !mv \"$TARGET_DIR/trainA\" \"$TARGET_DIR/train/A\"\n",
    "# !mv \"$TARGET_DIR/trainB\" \"$TARGET_DIR/train/B\"\n",
    "# !mv \"$TARGET_DIR/testA\" \"$TARGET_DIR/test/A\"\n",
    "# !mv \"$TARGET_DIR/testB\" \"$TARGET_DIR/test/B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=0, n_epochs=200, batchSize=2, dataroot='../dataset/horse2zebra/', lr=0.0002, decay_epoch=100, size=128, input_nc=3, output_nc=3, cuda=True, n_cpu=8)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
    "parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "parser.add_argument('--batchSize', type=int, default=2, help='size of the batches')\n",
    "parser.add_argument('--dataroot', type=str, default='../dataset/horse2zebra/', help='root directory of the dataset')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='epoch to start linearly decaying the learning rate to 0')\n",
    "parser.add_argument('--size', type=int, default=128, help='size of the data crop (squared assumed)')\n",
    "parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')\n",
    "parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')\n",
    "parser.add_argument('--cuda', default=True, action='store_true', help='use GPU computation')\n",
    "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "opt = parser.parse_args('')\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "{torch.Size([1, 256, 256]), torch.Size([3, 256, 256])}\n",
      "['n02391049_226.jpg', 'n02391049_2757.jpg', 'n02391049_2341.jpg', 'n02391049_7503.jpg', 'n02391049_2361.jpg', 'n02391049_8008.jpg', 'n02391049_9726.jpg']\n"
     ]
    }
   ],
   "source": [
    "myset = set()\n",
    "files = []\n",
    "for filename in os.listdir(os.path.join(TARGET_DIR, 'train', 'B')):\n",
    "    s = transforms.ToTensor()(Image.open(os.path.join(TARGET_DIR, 'train', 'B', filename))).shape\n",
    "    myset.add(s)\n",
    "    if s[0] == 1:\n",
    "        files.append(filename)\n",
    "        print(transforms.ToTensor()(Image.open(os.path.join(TARGET_DIR, 'train', 'B', filename)).convert('RGB')).shape)\n",
    "print(myset)\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%s/A' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%s/B' % mode) + '/*.*'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]).convert('RGB'))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]).convert('RGB'))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]).convert('RGB'))\n",
    "\n",
    "        return {'A': item_A, 'B': item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel) -> None:\n",
    "        super(GenConvolutionBlock, self).__init__()\n",
    "        padding = kernel // 2\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, stride=1, padding=padding, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_last_layer=False) -> None:\n",
    "        super(DisConvolutionBlock, self).__init__()\n",
    "        out_channels = 1 if is_last_layer else out_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            *[nn.Sigmoid()]\n",
    "            if is_last_layer\n",
    "            else [nn.InstanceNorm2d(out_channels), nn.LeakyReLU(0.02, inplace=True)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect'),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): GenConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DownsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DownsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (6): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (7): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (8): ResidualBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      )\n",
      "    )\n",
      "    (9): UpsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (10): UpsamplingBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (11): GenConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "generator_layers_config = [\n",
    "    \"c7s1-64\",\n",
    "    \"d128\",\n",
    "    \"d256\",\n",
    "    *(\"R256\" for _ in range(6)),\n",
    "    \"u128\",\n",
    "    \"u64\",\n",
    "    \"c7s1-3\",\n",
    "]\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, layers_config=generator_layers_config):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Generator Model\n",
    "        model = []\n",
    "        in_channels = input_nc\n",
    "        for layer_config in layers_config:\n",
    "            type = layer_config[0:1]\n",
    "            if type == \"c\":\n",
    "                kernel_size, _, out_channels = (\n",
    "                    int(outs)\n",
    "                    for outs in re.match(\"c(.+)s(.+)-(.+)\", layer_config).groups()\n",
    "                )\n",
    "                model.append(GenConvolutionBlock(in_channels, out_channels, kernel_size))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"d\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(DownsamplingBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"R\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "            elif type == \"u\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(UpsamplingBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "generator = Generator(3, 3)\n",
    "print(generator)\n",
    "generator(torch.zeros((1, 3, 256, 256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.02, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.02, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.02, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.02, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): DisConvolutionBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_layers_config = [\n",
    "    \"C64\",\n",
    "    \"C128\",\n",
    "    \"C256\",\n",
    "    \"C512\",\n",
    "]\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, layers_config=discriminator_layers_config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Discriminator Model\n",
    "        model = []\n",
    "        in_channels = input_nc\n",
    "        for layer_config in layers_config:\n",
    "            type = layer_config[0:1]\n",
    "            if type == \"C\":\n",
    "                out_channels = int(layer_config[1:])\n",
    "                model.append(DisConvolutionBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "        model.append(DisConvolutionBlock(in_channels, 1, True))\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "\n",
    "discriminator = Discriminator(3)\n",
    "print(discriminator)\n",
    "discriminator(torch.zeros(4, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lossess\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
    "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
    "netD_A = Discriminator(opt.input_nc)\n",
    "netD_B = Discriminator(opt.output_nc)\n",
    "\n",
    "if opt.cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "# netG_A2B.apply(weights_init_normal)\n",
    "# netG_B2A.apply(weights_init_normal)\n",
    "# netD_A.apply(weights_init_normal)\n",
    "# netD_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
    "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader\n",
    "transforms_ = [ transforms.Resize(int(opt.size*1.12), Image.BICUBIC), \n",
    "                transforms.RandomCrop(opt.size), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True), \n",
    "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/200 batch:667/667 | ra:0.9673 | fa:0.1661 | rb:0.9573 | fb:0.1524 | lg:11.6601 | la:0.0149 | lb:0.0179\n",
      "  pred_real_A:0.9673 | pred_fake_A:0.1661\n",
      "  pred_real_B:0.9573 | pred_fake_B:0.1524\n",
      "  loss_G:11.6601 | loss_D_A:0.0149 | loss_D_B:0.0179\n",
      "epoch:2/200 batch:667/667 | ra:0.6800 | fa:0.2603 | rb:0.9227 | fb:0.2603 | lg:9.6157 | la:0.1116 | lb:0.05614\n",
      "  pred_real_A:0.6800 | pred_fake_A:0.2603\n",
      "  pred_real_B:0.9227 | pred_fake_B:0.2603\n",
      "  loss_G:9.6157 | loss_D_A:0.1116 | loss_D_B:0.0561\n",
      "epoch:3/200 batch:667/667 | ra:0.7711 | fa:0.3364 | rb:0.9158 | fb:0.1115 | lg:11.4581 | la:0.0889 | lb:0.0109\n",
      "  pred_real_A:0.7711 | pred_fake_A:0.3364\n",
      "  pred_real_B:0.9158 | pred_fake_B:0.1115\n",
      "  loss_G:11.4581 | loss_D_A:0.0889 | loss_D_B:0.0109\n",
      "epoch:4/200 batch:667/667 | ra:0.8181 | fa:0.2332 | rb:0.9824 | fb:0.0741 | lg:14.8679 | la:0.0443 | lb:0.0032\n",
      "  pred_real_A:0.8181 | pred_fake_A:0.2332\n",
      "  pred_real_B:0.9824 | pred_fake_B:0.0741\n",
      "  loss_G:14.8679 | loss_D_A:0.0443 | loss_D_B:0.0032\n",
      "epoch:5/200 batch:667/667 | ra:0.5563 | fa:0.3793 | rb:0.9866 | fb:0.0652 | lg:14.3077 | la:0.2002 | lb:0.0039\n",
      "  pred_real_A:0.5563 | pred_fake_A:0.3793\n",
      "  pred_real_B:0.9866 | pred_fake_B:0.0652\n",
      "  loss_G:14.3077 | loss_D_A:0.2002 | loss_D_B:0.0039\n",
      "epoch:6/200 batch:667/667 | ra:0.5019 | fa:0.2043 | rb:0.9498 | fb:0.0315 | lg:14.8953 | la:0.1480 | lb:0.0032\n",
      "  pred_real_A:0.5019 | pred_fake_A:0.2043\n",
      "  pred_real_B:0.9498 | pred_fake_B:0.0315\n",
      "  loss_G:14.8953 | loss_D_A:0.1480 | loss_D_B:0.0032\n",
      "epoch:7/200 batch:667/667 | ra:0.7578 | fa:0.1224 | rb:0.8829 | fb:0.0643 | lg:14.4130 | la:0.0384 | lb:0.0103\n",
      "  pred_real_A:0.7578 | pred_fake_A:0.1224\n",
      "  pred_real_B:0.8829 | pred_fake_B:0.0643\n",
      "  loss_G:14.4130 | loss_D_A:0.0384 | loss_D_B:0.0103\n",
      "epoch:8/200 batch:667/667 | ra:0.7999 | fa:0.1969 | rb:0.9930 | fb:0.0023 | lg:11.6295 | la:0.0396 | lb:0.0000\n",
      "  pred_real_A:0.7999 | pred_fake_A:0.1969\n",
      "  pred_real_B:0.9930 | pred_fake_B:0.0023\n",
      "  loss_G:11.6295 | loss_D_A:0.0396 | loss_D_B:0.0000\n",
      "epoch:9/200 batch:667/667 | ra:0.8246 | fa:0.2723 | rb:0.8969 | fb:0.3759 | lg:10.6466 | la:0.0679 | lb:0.1269\n",
      "  pred_real_A:0.8246 | pred_fake_A:0.2723\n",
      "  pred_real_B:0.8969 | pred_fake_B:0.3759\n",
      "  loss_G:10.6466 | loss_D_A:0.0679 | loss_D_B:0.1269\n",
      "epoch:10/200 batch:667/667 | ra:0.7151 | fa:0.0579 | rb:0.9698 | fb:0.0417 | lg:13.6444 | la:0.0740 | lb:0.0024\n",
      "  pred_real_A:0.7151 | pred_fake_A:0.0579\n",
      "  pred_real_B:0.9698 | pred_fake_B:0.0417\n",
      "  loss_G:13.6444 | loss_D_A:0.0740 | loss_D_B:0.0024\n",
      "epoch:11/200 batch:667/667 | ra:0.3081 | fa:0.0601 | rb:0.7063 | fb:0.0013 | lg:15.7541 | la:0.2574 | lb:0.0439\n",
      "  pred_real_A:0.3081 | pred_fake_A:0.0601\n",
      "  pred_real_B:0.7063 | pred_fake_B:0.0013\n",
      "  loss_G:15.7541 | loss_D_A:0.2574 | loss_D_B:0.0439\n",
      "epoch:12/200 batch:667/667 | ra:0.6754 | fa:0.1720 | rb:0.9491 | fb:0.0050 | lg:13.6260 | la:0.0685 | lb:0.0017\n",
      "  pred_real_A:0.6754 | pred_fake_A:0.1720\n",
      "  pred_real_B:0.9491 | pred_fake_B:0.0050\n",
      "  loss_G:13.6260 | loss_D_A:0.0685 | loss_D_B:0.0017\n",
      "epoch:13/200 batch:667/667 | ra:0.9308 | fa:0.3034 | rb:0.9989 | fb:0.1099 | lg:11.0730 | la:0.0699 | lb:0.0120\n",
      "  pred_real_A:0.9308 | pred_fake_A:0.3034\n",
      "  pred_real_B:0.9989 | pred_fake_B:0.1099\n",
      "  loss_G:11.0730 | loss_D_A:0.0699 | loss_D_B:0.0120\n",
      "epoch:14/200 batch:667/667 | ra:0.9864 | fa:0.1666 | rb:0.9984 | fb:0.1471 | lg:12.0071 | la:0.0158 | lb:0.0111\n",
      "  pred_real_A:0.9864 | pred_fake_A:0.1666\n",
      "  pred_real_B:0.9984 | pred_fake_B:0.1471\n",
      "  loss_G:12.0071 | loss_D_A:0.0158 | loss_D_B:0.0111\n",
      "epoch:15/200 batch:667/667 | ra:0.6631 | fa:0.4074 | rb:0.9813 | fb:0.0037 | lg:12.8780 | la:0.1585 | lb:0.0003\n",
      "  pred_real_A:0.6631 | pred_fake_A:0.4074\n",
      "  pred_real_B:0.9813 | pred_fake_B:0.0037\n",
      "  loss_G:12.8780 | loss_D_A:0.1585 | loss_D_B:0.0003\n",
      "epoch:16/200 batch:667/667 | ra:0.9998 | fa:0.1114 | rb:0.9746 | fb:0.1472 | lg:10.1706 | la:0.0067 | lb:0.0167\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.1114\n",
      "  pred_real_B:0.9746 | pred_fake_B:0.1472\n",
      "  loss_G:10.1706 | loss_D_A:0.0067 | loss_D_B:0.0167\n",
      "epoch:17/200 batch:667/667 | ra:0.8723 | fa:0.1508 | rb:0.9965 | fb:0.0114 | lg:13.8411 | la:0.0334 | lb:0.0001\n",
      "  pred_real_A:0.8723 | pred_fake_A:0.1508\n",
      "  pred_real_B:0.9965 | pred_fake_B:0.0114\n",
      "  loss_G:13.8411 | loss_D_A:0.0334 | loss_D_B:0.0001\n",
      "epoch:18/200 batch:667/667 | ra:0.9312 | fa:0.3935 | rb:0.9859 | fb:0.2975 | lg:10.1860 | la:0.0830 | lb:0.0445\n",
      "  pred_real_A:0.9312 | pred_fake_A:0.3935\n",
      "  pred_real_B:0.9859 | pred_fake_B:0.2975\n",
      "  loss_G:10.1860 | loss_D_A:0.0830 | loss_D_B:0.0445\n",
      "epoch:19/200 batch:667/667 | ra:0.8790 | fa:0.1324 | rb:0.9863 | fb:0.1332 | lg:15.1395 | la:0.0293 | lb:0.0178\n",
      "  pred_real_A:0.8790 | pred_fake_A:0.1324\n",
      "  pred_real_B:0.9863 | pred_fake_B:0.1332\n",
      "  loss_G:15.1395 | loss_D_A:0.0293 | loss_D_B:0.0178\n",
      "epoch:20/200 batch:667/667 | ra:0.5784 | fa:0.3121 | rb:0.9945 | fb:0.0009 | lg:13.5140 | la:0.1794 | lb:0.0000\n",
      "  pred_real_A:0.5784 | pred_fake_A:0.3121\n",
      "  pred_real_B:0.9945 | pred_fake_B:0.0009\n",
      "  loss_G:13.5140 | loss_D_A:0.1794 | loss_D_B:0.0000\n",
      "epoch:21/200 batch:667/667 | ra:0.9957 | fa:0.1029 | rb:0.9816 | fb:0.0036 | lg:10.7376 | la:0.0079 | lb:0.0003\n",
      "  pred_real_A:0.9957 | pred_fake_A:0.1029\n",
      "  pred_real_B:0.9816 | pred_fake_B:0.0036\n",
      "  loss_G:10.7376 | loss_D_A:0.0079 | loss_D_B:0.0003\n",
      "epoch:22/200 batch:667/667 | ra:0.7260 | fa:0.2320 | rb:0.9792 | fb:0.0024 | lg:11.7962 | la:0.1247 | lb:0.0004\n",
      "  pred_real_A:0.7260 | pred_fake_A:0.2320\n",
      "  pred_real_B:0.9792 | pred_fake_B:0.0024\n",
      "  loss_G:11.7962 | loss_D_A:0.1247 | loss_D_B:0.0004\n",
      "epoch:23/200 batch:667/667 | ra:0.8743 | fa:0.4208 | rb:0.9957 | fb:0.0003 | lg:10.2624 | la:0.1118 | lb:0.0000\n",
      "  pred_real_A:0.8743 | pred_fake_A:0.4208\n",
      "  pred_real_B:0.9957 | pred_fake_B:0.0003\n",
      "  loss_G:10.2624 | loss_D_A:0.1118 | loss_D_B:0.0000\n",
      "epoch:24/200 batch:667/667 | ra:0.9300 | fa:0.1858 | rb:0.9959 | fb:0.1123 | lg:10.7627 | la:0.0362 | lb:0.0072\n",
      "  pred_real_A:0.9300 | pred_fake_A:0.1858\n",
      "  pred_real_B:0.9959 | pred_fake_B:0.1123\n",
      "  loss_G:10.7627 | loss_D_A:0.0362 | loss_D_B:0.0072\n",
      "epoch:25/200 batch:667/667 | ra:0.8338 | fa:0.1160 | rb:0.9961 | fb:0.0854 | lg:13.2029 | la:0.0374 | lb:0.0065\n",
      "  pred_real_A:0.8338 | pred_fake_A:0.1160\n",
      "  pred_real_B:0.9961 | pred_fake_B:0.0854\n",
      "  loss_G:13.2029 | loss_D_A:0.0374 | loss_D_B:0.0065\n",
      "epoch:26/200 batch:667/667 | ra:0.9991 | fa:0.0037 | rb:0.9658 | fb:0.0017 | lg:10.6878 | la:0.0000 | lb:0.0012\n",
      "  pred_real_A:0.9991 | pred_fake_A:0.0037\n",
      "  pred_real_B:0.9658 | pred_fake_B:0.0017\n",
      "  loss_G:10.6878 | loss_D_A:0.0000 | loss_D_B:0.0012\n",
      "epoch:27/200 batch:667/667 | ra:0.6964 | fa:0.2062 | rb:0.9975 | fb:0.0002 | lg:12.0875 | la:0.0966 | lb:0.0000\n",
      "  pred_real_A:0.6964 | pred_fake_A:0.2062\n",
      "  pred_real_B:0.9975 | pred_fake_B:0.0002\n",
      "  loss_G:12.0875 | loss_D_A:0.0966 | loss_D_B:0.0000\n",
      "epoch:28/200 batch:667/667 | ra:0.9208 | fa:0.0771 | rb:0.9956 | fb:0.0042 | lg:11.2064 | la:0.0070 | lb:0.0000\n",
      "  pred_real_A:0.9208 | pred_fake_A:0.0771\n",
      "  pred_real_B:0.9956 | pred_fake_B:0.0042\n",
      "  loss_G:11.2064 | loss_D_A:0.0070 | loss_D_B:0.0000\n",
      "epoch:29/200 batch:667/667 | ra:0.9768 | fa:0.1620 | rb:0.9982 | fb:0.0328 | lg:12.1852 | la:0.0135 | lb:0.0011\n",
      "  pred_real_A:0.9768 | pred_fake_A:0.1620\n",
      "  pred_real_B:0.9982 | pred_fake_B:0.0328\n",
      "  loss_G:12.1852 | loss_D_A:0.0135 | loss_D_B:0.0011\n",
      "epoch:30/200 batch:667/667 | ra:0.9826 | fa:0.3408 | rb:1.0000 | fb:0.0067 | lg:11.4017 | la:0.0699 | lb:0.0000\n",
      "  pred_real_A:0.9826 | pred_fake_A:0.3408\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0067\n",
      "  loss_G:11.4017 | loss_D_A:0.0699 | loss_D_B:0.0000\n",
      "epoch:31/200 batch:667/667 | ra:0.9711 | fa:0.0406 | rb:0.9986 | fb:0.0043 | lg:10.5813 | la:0.0014 | lb:0.0000\n",
      "  pred_real_A:0.9711 | pred_fake_A:0.0406\n",
      "  pred_real_B:0.9986 | pred_fake_B:0.0043\n",
      "  loss_G:10.5813 | loss_D_A:0.0014 | loss_D_B:0.0000\n",
      "epoch:32/200 batch:667/667 | ra:0.9648 | fa:0.1804 | rb:0.9487 | fb:0.0740 | lg:12.0227 | la:0.0223 | lb:0.0061\n",
      "  pred_real_A:0.9648 | pred_fake_A:0.1804\n",
      "  pred_real_B:0.9487 | pred_fake_B:0.0740\n",
      "  loss_G:12.0227 | loss_D_A:0.0223 | loss_D_B:0.0061\n",
      "epoch:33/200 batch:667/667 | ra:0.7903 | fa:0.0004 | rb:0.9999 | fb:0.0025 | lg:15.2988 | la:0.0439 | lb:0.0000\n",
      "  pred_real_A:0.7903 | pred_fake_A:0.0004\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0025\n",
      "  loss_G:15.2988 | loss_D_A:0.0439 | loss_D_B:0.0000\n",
      "epoch:34/200 batch:667/667 | ra:0.9310 | fa:0.4004 | rb:0.9996 | fb:0.0006 | lg:12.2886 | la:0.1204 | lb:0.0000\n",
      "  pred_real_A:0.9310 | pred_fake_A:0.4004\n",
      "  pred_real_B:0.9996 | pred_fake_B:0.0006\n",
      "  loss_G:12.2886 | loss_D_A:0.1204 | loss_D_B:0.0000\n",
      "epoch:35/200 batch:667/667 | ra:0.9457 | fa:0.3044 | rb:0.9658 | fb:0.0001 | lg:11.0581 | la:0.0515 | lb:0.0012\n",
      "  pred_real_A:0.9457 | pred_fake_A:0.3044\n",
      "  pred_real_B:0.9658 | pred_fake_B:0.0001\n",
      "  loss_G:11.0581 | loss_D_A:0.0515 | loss_D_B:0.0012\n",
      "epoch:36/200 batch:667/667 | ra:0.8068 | fa:0.1317 | rb:0.8796 | fb:0.0021 | lg:8.5162 | la:0.0476 | lb:0.01450\n",
      "  pred_real_A:0.8068 | pred_fake_A:0.1317\n",
      "  pred_real_B:0.8796 | pred_fake_B:0.0021\n",
      "  loss_G:8.5162 | loss_D_A:0.0476 | loss_D_B:0.0145\n",
      "epoch:37/200 batch:667/667 | ra:0.9956 | fa:0.0243 | rb:0.9491 | fb:0.1384 | lg:11.7696 | la:0.0006 | lb:0.0162\n",
      "  pred_real_A:0.9956 | pred_fake_A:0.0243\n",
      "  pred_real_B:0.9491 | pred_fake_B:0.1384\n",
      "  loss_G:11.7696 | loss_D_A:0.0006 | loss_D_B:0.0162\n",
      "epoch:38/200 batch:667/667 | ra:0.9896 | fa:0.0462 | rb:0.9326 | fb:0.0565 | lg:11.4156 | la:0.0020 | lb:0.0047\n",
      "  pred_real_A:0.9896 | pred_fake_A:0.0462\n",
      "  pred_real_B:0.9326 | pred_fake_B:0.0565\n",
      "  loss_G:11.4156 | loss_D_A:0.0020 | loss_D_B:0.0047\n",
      "epoch:39/200 batch:667/667 | ra:0.6219 | fa:0.2053 | rb:0.9999 | fb:0.0000 | lg:11.2418 | la:0.1034 | lb:0.0000\n",
      "  pred_real_A:0.6219 | pred_fake_A:0.2053\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.2418 | loss_D_A:0.1034 | loss_D_B:0.0000\n",
      "epoch:40/200 batch:667/667 | ra:0.9280 | fa:0.3014 | rb:0.9997 | fb:0.0000 | lg:12.0795 | la:0.0807 | lb:0.0000\n",
      "  pred_real_A:0.9280 | pred_fake_A:0.3014\n",
      "  pred_real_B:0.9997 | pred_fake_B:0.0000\n",
      "  loss_G:12.0795 | loss_D_A:0.0807 | loss_D_B:0.0000\n",
      "epoch:41/200 batch:667/667 | ra:1.0000 | fa:0.2243 | rb:0.9681 | fb:0.3235 | lg:8.6915 | la:0.0496 | lb:0.10126\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.2243\n",
      "  pred_real_B:0.9681 | pred_fake_B:0.3235\n",
      "  loss_G:8.6915 | loss_D_A:0.0496 | loss_D_B:0.1012\n",
      "epoch:42/200 batch:667/667 | ra:0.9540 | fa:0.0115 | rb:0.9989 | fb:0.0042 | lg:11.5850 | la:0.0021 | lb:0.0000\n",
      "  pred_real_A:0.9540 | pred_fake_A:0.0115\n",
      "  pred_real_B:0.9989 | pred_fake_B:0.0042\n",
      "  loss_G:11.5850 | loss_D_A:0.0021 | loss_D_B:0.0000\n",
      "epoch:43/200 batch:667/667 | ra:0.6909 | fa:0.4819 | rb:0.9999 | fb:0.0000 | lg:13.5100 | la:0.1659 | lb:0.0000\n",
      "  pred_real_A:0.6909 | pred_fake_A:0.4819\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:13.5100 | loss_D_A:0.1659 | loss_D_B:0.0000\n",
      "epoch:44/200 batch:667/667 | ra:0.8808 | fa:0.0015 | rb:0.9999 | fb:0.0000 | lg:10.8154 | la:0.0128 | lb:0.0000\n",
      "  pred_real_A:0.8808 | pred_fake_A:0.0015\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:10.8154 | loss_D_A:0.0128 | loss_D_B:0.0000\n",
      "epoch:45/200 batch:667/667 | ra:0.8647 | fa:0.1523 | rb:1.0000 | fb:0.0000 | lg:11.6932 | la:0.0409 | lb:0.0000\n",
      "  pred_real_A:0.8647 | pred_fake_A:0.1523\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.6932 | loss_D_A:0.0409 | loss_D_B:0.0000\n",
      "epoch:46/200 batch:667/667 | ra:0.9966 | fa:0.1527 | rb:0.9994 | fb:0.0305 | lg:10.2845 | la:0.0225 | lb:0.0009\n",
      "  pred_real_A:0.9966 | pred_fake_A:0.1527\n",
      "  pred_real_B:0.9994 | pred_fake_B:0.0305\n",
      "  loss_G:10.2845 | loss_D_A:0.0225 | loss_D_B:0.0009\n",
      "epoch:47/200 batch:667/667 | ra:0.9442 | fa:0.0017 | rb:1.0000 | fb:0.0022 | lg:9.4707 | la:0.0031 | lb:0.00004\n",
      "  pred_real_A:0.9442 | pred_fake_A:0.0017\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0022\n",
      "  loss_G:9.4707 | loss_D_A:0.0031 | loss_D_B:0.0000\n",
      "epoch:48/200 batch:667/667 | ra:0.5726 | fa:0.0105 | rb:0.9999 | fb:0.0030 | lg:13.3404 | la:0.1420 | lb:0.0000\n",
      "  pred_real_A:0.5726 | pred_fake_A:0.0105\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0030\n",
      "  loss_G:13.3404 | loss_D_A:0.1420 | loss_D_B:0.0000\n",
      "epoch:49/200 batch:667/667 | ra:0.6736 | fa:0.0662 | rb:0.9666 | fb:0.0001 | lg:9.3377 | la:0.0898 | lb:0.00068\n",
      "  pred_real_A:0.6736 | pred_fake_A:0.0662\n",
      "  pred_real_B:0.9666 | pred_fake_B:0.0001\n",
      "  loss_G:9.3377 | loss_D_A:0.0898 | loss_D_B:0.0006\n",
      "epoch:50/200 batch:667/667 | ra:0.9439 | fa:0.0291 | rb:0.8569 | fb:0.0026 | lg:12.2395 | la:0.0031 | lb:0.0204\n",
      "  pred_real_A:0.9439 | pred_fake_A:0.0291\n",
      "  pred_real_B:0.8569 | pred_fake_B:0.0026\n",
      "  loss_G:12.2395 | loss_D_A:0.0031 | loss_D_B:0.0204\n",
      "epoch:51/200 batch:667/667 | ra:1.0000 | fa:0.5006 | rb:0.9991 | fb:0.0001 | lg:9.4149 | la:0.1356 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.5006\n",
      "  pred_real_B:0.9991 | pred_fake_B:0.0001\n",
      "  loss_G:9.4149 | loss_D_A:0.1356 | loss_D_B:0.0000\n",
      "epoch:52/200 batch:667/667 | ra:0.9572 | fa:0.0332 | rb:0.9963 | fb:0.0269 | lg:10.2027 | la:0.0022 | lb:0.0007\n",
      "  pred_real_A:0.9572 | pred_fake_A:0.0332\n",
      "  pred_real_B:0.9963 | pred_fake_B:0.0269\n",
      "  loss_G:10.2027 | loss_D_A:0.0022 | loss_D_B:0.0007\n",
      "epoch:53/200 batch:667/667 | ra:0.9596 | fa:0.0341 | rb:0.9921 | fb:0.2047 | lg:10.9496 | la:0.0026 | lb:0.0417\n",
      "  pred_real_A:0.9596 | pred_fake_A:0.0341\n",
      "  pred_real_B:0.9921 | pred_fake_B:0.2047\n",
      "  loss_G:10.9496 | loss_D_A:0.0026 | loss_D_B:0.0417\n",
      "epoch:54/200 batch:667/667 | ra:0.8971 | fa:0.0115 | rb:0.9987 | fb:0.0359 | lg:15.1697 | la:0.0056 | lb:0.0011\n",
      "  pred_real_A:0.8971 | pred_fake_A:0.0115\n",
      "  pred_real_B:0.9987 | pred_fake_B:0.0359\n",
      "  loss_G:15.1697 | loss_D_A:0.0056 | loss_D_B:0.0011\n",
      "epoch:55/200 batch:667/667 | ra:0.9641 | fa:0.0905 | rb:0.9892 | fb:0.0001 | lg:11.9463 | la:0.0094 | lb:0.0001\n",
      "  pred_real_A:0.9641 | pred_fake_A:0.0905\n",
      "  pred_real_B:0.9892 | pred_fake_B:0.0001\n",
      "  loss_G:11.9463 | loss_D_A:0.0094 | loss_D_B:0.0001\n",
      "epoch:56/200 batch:667/667 | ra:0.9685 | fa:0.1460 | rb:0.9707 | fb:0.0000 | lg:12.2176 | la:0.0216 | lb:0.0008\n",
      "  pred_real_A:0.9685 | pred_fake_A:0.1460\n",
      "  pred_real_B:0.9707 | pred_fake_B:0.0000\n",
      "  loss_G:12.2176 | loss_D_A:0.0216 | loss_D_B:0.0008\n",
      "epoch:57/200 batch:667/667 | ra:0.9879 | fa:0.0000 | rb:0.9999 | fb:0.0045 | lg:10.1964 | la:0.0001 | lb:0.0000\n",
      "  pred_real_A:0.9879 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0045\n",
      "  loss_G:10.1964 | loss_D_A:0.0001 | loss_D_B:0.0000\n",
      "epoch:58/200 batch:667/667 | ra:0.9828 | fa:0.0261 | rb:0.9993 | fb:0.0259 | lg:12.9617 | la:0.0009 | lb:0.0007\n",
      "  pred_real_A:0.9828 | pred_fake_A:0.0261\n",
      "  pred_real_B:0.9993 | pred_fake_B:0.0259\n",
      "  loss_G:12.9617 | loss_D_A:0.0009 | loss_D_B:0.0007\n",
      "epoch:59/200 batch:667/667 | ra:0.5748 | fa:0.0042 | rb:1.0000 | fb:0.0000 | lg:12.7067 | la:0.1440 | lb:0.0000\n",
      "  pred_real_A:0.5748 | pred_fake_A:0.0042\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.7067 | loss_D_A:0.1440 | loss_D_B:0.0000\n",
      "epoch:60/200 batch:667/667 | ra:0.9727 | fa:0.0010 | rb:0.9996 | fb:0.0024 | lg:11.4734 | la:0.0007 | lb:0.0000\n",
      "  pred_real_A:0.9727 | pred_fake_A:0.0010\n",
      "  pred_real_B:0.9996 | pred_fake_B:0.0024\n",
      "  loss_G:11.4734 | loss_D_A:0.0007 | loss_D_B:0.0000\n",
      "epoch:61/200 batch:667/667 | ra:0.9995 | fa:0.0013 | rb:0.9830 | fb:0.0278 | lg:13.3048 | la:0.0000 | lb:0.0011\n",
      "  pred_real_A:0.9995 | pred_fake_A:0.0013\n",
      "  pred_real_B:0.9830 | pred_fake_B:0.0278\n",
      "  loss_G:13.3048 | loss_D_A:0.0000 | loss_D_B:0.0011\n",
      "epoch:62/200 batch:667/667 | ra:0.9066 | fa:0.3319 | rb:0.9796 | fb:0.0010 | lg:9.6170 | la:0.1142 | lb:0.00048\n",
      "  pred_real_A:0.9066 | pred_fake_A:0.3319\n",
      "  pred_real_B:0.9796 | pred_fake_B:0.0010\n",
      "  loss_G:9.6170 | loss_D_A:0.1142 | loss_D_B:0.0004\n",
      "epoch:63/200 batch:667/667 | ra:0.9930 | fa:0.0017 | rb:0.9999 | fb:0.0001 | lg:10.6535 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9930 | pred_fake_A:0.0017\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0001\n",
      "  loss_G:10.6535 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:64/200 batch:667/667 | ra:0.9070 | fa:0.0620 | rb:0.9988 | fb:0.0000 | lg:12.5280 | la:0.0117 | lb:0.0000\n",
      "  pred_real_A:0.9070 | pred_fake_A:0.0620\n",
      "  pred_real_B:0.9988 | pred_fake_B:0.0000\n",
      "  loss_G:12.5280 | loss_D_A:0.0117 | loss_D_B:0.0000\n",
      "epoch:65/200 batch:667/667 | ra:0.9836 | fa:0.0573 | rb:0.9753 | fb:0.0000 | lg:14.9809 | la:0.0034 | lb:0.0006\n",
      "  pred_real_A:0.9836 | pred_fake_A:0.0573\n",
      "  pred_real_B:0.9753 | pred_fake_B:0.0000\n",
      "  loss_G:14.9809 | loss_D_A:0.0034 | loss_D_B:0.0006\n",
      "epoch:66/200 batch:667/667 | ra:0.9997 | fa:0.0398 | rb:0.9991 | fb:0.0587 | lg:12.7060 | la:0.0016 | lb:0.0034\n",
      "  pred_real_A:0.9997 | pred_fake_A:0.0398\n",
      "  pred_real_B:0.9991 | pred_fake_B:0.0587\n",
      "  loss_G:12.7060 | loss_D_A:0.0016 | loss_D_B:0.0034\n",
      "epoch:67/200 batch:667/667 | ra:0.9877 | fa:0.1967 | rb:0.9842 | fb:0.0002 | lg:13.4406 | la:0.0359 | lb:0.0003\n",
      "  pred_real_A:0.9877 | pred_fake_A:0.1967\n",
      "  pred_real_B:0.9842 | pred_fake_B:0.0002\n",
      "  loss_G:13.4406 | loss_D_A:0.0359 | loss_D_B:0.0003\n",
      "epoch:68/200 batch:667/667 | ra:1.0000 | fa:0.4441 | rb:1.0000 | fb:0.0014 | lg:15.0292 | la:0.1116 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.4441\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0014\n",
      "  loss_G:15.0292 | loss_D_A:0.1116 | loss_D_B:0.0000\n",
      "epoch:69/200 batch:667/667 | ra:0.9310 | fa:0.1348 | rb:1.0000 | fb:0.0000 | lg:8.9071 | la:0.0226 | lb:0.00000\n",
      "  pred_real_A:0.9310 | pred_fake_A:0.1348\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.9071 | loss_D_A:0.0226 | loss_D_B:0.0000\n",
      "epoch:70/200 batch:667/667 | ra:0.9912 | fa:0.1058 | rb:0.9998 | fb:0.0001 | lg:11.0717 | la:0.0074 | lb:0.0000\n",
      "  pred_real_A:0.9912 | pred_fake_A:0.1058\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0001\n",
      "  loss_G:11.0717 | loss_D_A:0.0074 | loss_D_B:0.0000\n",
      "epoch:71/200 batch:667/667 | ra:0.9989 | fa:0.0714 | rb:0.9999 | fb:0.0000 | lg:10.5109 | la:0.0049 | lb:0.0000\n",
      "  pred_real_A:0.9989 | pred_fake_A:0.0714\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:10.5109 | loss_D_A:0.0049 | loss_D_B:0.0000\n",
      "epoch:72/200 batch:667/667 | ra:0.9301 | fa:0.2885 | rb:1.0000 | fb:0.0000 | lg:10.4187 | la:0.0697 | lb:0.0000\n",
      "  pred_real_A:0.9301 | pred_fake_A:0.2885\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.4187 | loss_D_A:0.0697 | loss_D_B:0.0000\n",
      "epoch:73/200 batch:667/667 | ra:1.0000 | fa:0.0012 | rb:0.8884 | fb:0.1208 | lg:9.3022 | la:0.0000 | lb:0.02701\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0012\n",
      "  pred_real_B:0.8884 | pred_fake_B:0.1208\n",
      "  loss_G:9.3022 | loss_D_A:0.0000 | loss_D_B:0.0270\n",
      "epoch:74/200 batch:667/667 | ra:0.9753 | fa:0.0780 | rb:0.9997 | fb:0.0008 | lg:10.5687 | la:0.0061 | lb:0.0000\n",
      "  pred_real_A:0.9753 | pred_fake_A:0.0780\n",
      "  pred_real_B:0.9997 | pred_fake_B:0.0008\n",
      "  loss_G:10.5687 | loss_D_A:0.0061 | loss_D_B:0.0000\n",
      "epoch:75/200 batch:667/667 | ra:0.9995 | fa:0.2662 | rb:0.9852 | fb:0.0309 | lg:11.7415 | la:0.0539 | lb:0.0012\n",
      "  pred_real_A:0.9995 | pred_fake_A:0.2662\n",
      "  pred_real_B:0.9852 | pred_fake_B:0.0309\n",
      "  loss_G:11.7415 | loss_D_A:0.0539 | loss_D_B:0.0012\n",
      "epoch:76/200 batch:667/667 | ra:0.9675 | fa:0.0010 | rb:0.9798 | fb:0.0020 | lg:12.4544 | la:0.0011 | lb:0.0004\n",
      "  pred_real_A:0.9675 | pred_fake_A:0.0010\n",
      "  pred_real_B:0.9798 | pred_fake_B:0.0020\n",
      "  loss_G:12.4544 | loss_D_A:0.0011 | loss_D_B:0.0004\n",
      "epoch:77/200 batch:667/667 | ra:0.9049 | fa:0.3600 | rb:0.9998 | fb:0.0000 | lg:12.6630 | la:0.0735 | lb:0.0000\n",
      "  pred_real_A:0.9049 | pred_fake_A:0.3600\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:12.6630 | loss_D_A:0.0735 | loss_D_B:0.0000\n",
      "epoch:78/200 batch:667/667 | ra:0.9845 | fa:0.0600 | rb:1.0000 | fb:0.0017 | lg:9.6239 | la:0.0037 | lb:0.00000\n",
      "  pred_real_A:0.9845 | pred_fake_A:0.0600\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0017\n",
      "  loss_G:9.6239 | loss_D_A:0.0037 | loss_D_B:0.0000\n",
      "epoch:79/200 batch:667/667 | ra:0.9991 | fa:0.0096 | rb:1.0000 | fb:0.0735 | lg:9.5315 | la:0.0001 | lb:0.00540\n",
      "  pred_real_A:0.9991 | pred_fake_A:0.0096\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0735\n",
      "  loss_G:9.5315 | loss_D_A:0.0001 | loss_D_B:0.0054\n",
      "epoch:80/200 batch:667/667 | ra:0.9474 | fa:0.1196 | rb:1.0000 | fb:0.0000 | lg:10.9065 | la:0.0105 | lb:0.0000\n",
      "  pred_real_A:0.9474 | pred_fake_A:0.1196\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.9065 | loss_D_A:0.0105 | loss_D_B:0.0000\n",
      "epoch:81/200 batch:667/667 | ra:0.9999 | fa:0.0976 | rb:1.0000 | fb:0.0000 | lg:14.3016 | la:0.0050 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0976\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:14.3016 | loss_D_A:0.0050 | loss_D_B:0.0000\n",
      "epoch:82/200 batch:667/667 | ra:0.9998 | fa:0.1226 | rb:0.9999 | fb:0.0000 | lg:10.6213 | la:0.0076 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.1226\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:10.6213 | loss_D_A:0.0076 | loss_D_B:0.0000\n",
      "epoch:83/200 batch:667/667 | ra:1.0000 | fa:0.0969 | rb:1.0000 | fb:0.0000 | lg:8.9749 | la:0.0089 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0969\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.9749 | loss_D_A:0.0089 | loss_D_B:0.0000\n",
      "epoch:84/200 batch:667/667 | ra:0.9513 | fa:0.0010 | rb:1.0000 | fb:0.0000 | lg:11.8572 | la:0.0022 | lb:0.0000\n",
      "  pred_real_A:0.9513 | pred_fake_A:0.0010\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.8572 | loss_D_A:0.0022 | loss_D_B:0.0000\n",
      "epoch:85/200 batch:667/667 | ra:0.9748 | fa:0.1155 | rb:0.9961 | fb:0.0000 | lg:13.1163 | la:0.0101 | lb:0.0000\n",
      "  pred_real_A:0.9748 | pred_fake_A:0.1155\n",
      "  pred_real_B:0.9961 | pred_fake_B:0.0000\n",
      "  loss_G:13.1163 | loss_D_A:0.0101 | loss_D_B:0.0000\n",
      "epoch:86/200 batch:667/667 | ra:0.9960 | fa:0.0077 | rb:1.0000 | fb:0.0000 | lg:11.6261 | la:0.0001 | lb:0.0000\n",
      "  pred_real_A:0.9960 | pred_fake_A:0.0077\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.6261 | loss_D_A:0.0001 | loss_D_B:0.0000\n",
      "epoch:87/200 batch:667/667 | ra:0.9988 | fa:0.1314 | rb:0.9637 | fb:0.0000 | lg:9.4105 | la:0.0173 | lb:0.00130\n",
      "  pred_real_A:0.9988 | pred_fake_A:0.1314\n",
      "  pred_real_B:0.9637 | pred_fake_B:0.0000\n",
      "  loss_G:9.4105 | loss_D_A:0.0173 | loss_D_B:0.0013\n",
      "epoch:88/200 batch:667/667 | ra:0.9968 | fa:0.0071 | rb:0.9998 | fb:0.0000 | lg:10.2463 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9968 | pred_fake_A:0.0071\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:10.2463 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:89/200 batch:667/667 | ra:0.9938 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:12.8218 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9938 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.8218 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:90/200 batch:667/667 | ra:0.9724 | fa:0.0085 | rb:1.0000 | fb:0.0057 | lg:14.2133 | la:0.0008 | lb:0.0000\n",
      "  pred_real_A:0.9724 | pred_fake_A:0.0085\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0057\n",
      "  loss_G:14.2133 | loss_D_A:0.0008 | loss_D_B:0.0000\n",
      "epoch:91/200 batch:667/667 | ra:0.9403 | fa:0.0862 | rb:0.9790 | fb:0.0001 | lg:13.7772 | la:0.0075 | lb:0.0004\n",
      "  pred_real_A:0.9403 | pred_fake_A:0.0862\n",
      "  pred_real_B:0.9790 | pred_fake_B:0.0001\n",
      "  loss_G:13.7772 | loss_D_A:0.0075 | loss_D_B:0.0004\n",
      "epoch:92/200 batch:667/667 | ra:0.8050 | fa:0.0000 | rb:0.8790 | fb:0.0000 | lg:11.4888 | la:0.0288 | lb:0.0139\n",
      "  pred_real_A:0.8050 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.8790 | pred_fake_B:0.0000\n",
      "  loss_G:11.4888 | loss_D_A:0.0288 | loss_D_B:0.0139\n",
      "epoch:93/200 batch:667/667 | ra:0.9966 | fa:0.1964 | rb:1.0000 | fb:0.0000 | lg:9.7606 | la:0.0199 | lb:0.00000\n",
      "  pred_real_A:0.9966 | pred_fake_A:0.1964\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.7606 | loss_D_A:0.0199 | loss_D_B:0.0000\n",
      "epoch:94/200 batch:667/667 | ra:0.9998 | fa:0.0007 | rb:0.9999 | fb:0.0001 | lg:11.3604 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0007\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0001\n",
      "  loss_G:11.3604 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:95/200 batch:667/667 | ra:0.9070 | fa:0.0749 | rb:0.9997 | fb:0.0000 | lg:9.7763 | la:0.0136 | lb:0.00000\n",
      "  pred_real_A:0.9070 | pred_fake_A:0.0749\n",
      "  pred_real_B:0.9997 | pred_fake_B:0.0000\n",
      "  loss_G:9.7763 | loss_D_A:0.0136 | loss_D_B:0.0000\n",
      "epoch:96/200 batch:667/667 | ra:0.9281 | fa:0.0301 | rb:0.9995 | fb:0.0005 | lg:10.9291 | la:0.0058 | lb:0.0000\n",
      "  pred_real_A:0.9281 | pred_fake_A:0.0301\n",
      "  pred_real_B:0.9995 | pred_fake_B:0.0005\n",
      "  loss_G:10.9291 | loss_D_A:0.0058 | loss_D_B:0.0000\n",
      "epoch:97/200 batch:667/667 | ra:0.9279 | fa:0.0004 | rb:1.0000 | fb:0.0000 | lg:11.8336 | la:0.0046 | lb:0.0000\n",
      "  pred_real_A:0.9279 | pred_fake_A:0.0004\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.8336 | loss_D_A:0.0046 | loss_D_B:0.0000\n",
      "epoch:98/200 batch:667/667 | ra:0.6937 | fa:0.0403 | rb:0.9993 | fb:0.0528 | lg:13.8757 | la:0.0936 | lb:0.0014\n",
      "  pred_real_A:0.6937 | pred_fake_A:0.0403\n",
      "  pred_real_B:0.9993 | pred_fake_B:0.0528\n",
      "  loss_G:13.8757 | loss_D_A:0.0936 | loss_D_B:0.0014\n",
      "epoch:99/200 batch:667/667 | ra:0.9998 | fa:0.0002 | rb:1.0000 | fb:0.0001 | lg:11.7699 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0001\n",
      "  loss_G:11.7699 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:100/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:0.9998 | fb:0.0000 | lg:13.0120 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:13.0120 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:101/200 batch:667/667 | ra:1.0000 | fa:0.0840 | rb:0.9997 | fb:0.0013 | lg:11.1381 | la:0.0070 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0840\n",
      "  pred_real_B:0.9997 | pred_fake_B:0.0013\n",
      "  loss_G:11.1381 | loss_D_A:0.0070 | loss_D_B:0.0000\n",
      "epoch:102/200 batch:667/667 | ra:0.9092 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:11.2874 | la:0.0048 | lb:0.0000\n",
      "  pred_real_A:0.9092 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.2874 | loss_D_A:0.0048 | loss_D_B:0.0000\n",
      "epoch:103/200 batch:667/667 | ra:1.0000 | fa:0.0023 | rb:1.0000 | fb:0.0000 | lg:9.3273 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0023\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.3273 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:104/200 batch:667/667 | ra:0.9598 | fa:0.0772 | rb:1.0000 | fb:0.0038 | lg:9.1351 | la:0.0076 | lb:0.00000\n",
      "  pred_real_A:0.9598 | pred_fake_A:0.0772\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0038\n",
      "  loss_G:9.1351 | loss_D_A:0.0076 | loss_D_B:0.0000\n",
      "epoch:105/200 batch:667/667 | ra:0.9999 | fa:0.0591 | rb:0.9985 | fb:0.0001 | lg:11.0381 | la:0.0029 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0591\n",
      "  pred_real_B:0.9985 | pred_fake_B:0.0001\n",
      "  loss_G:11.0381 | loss_D_A:0.0029 | loss_D_B:0.0000\n",
      "epoch:106/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:10.3536 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.3536 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:107/200 batch:667/667 | ra:0.8977 | fa:0.0008 | rb:0.9977 | fb:0.0000 | lg:14.2361 | la:0.0071 | lb:0.0000\n",
      "  pred_real_A:0.8977 | pred_fake_A:0.0008\n",
      "  pred_real_B:0.9977 | pred_fake_B:0.0000\n",
      "  loss_G:14.2361 | loss_D_A:0.0071 | loss_D_B:0.0000\n",
      "epoch:108/200 batch:667/667 | ra:0.9858 | fa:0.0601 | rb:1.0000 | fb:0.0000 | lg:10.7483 | la:0.0037 | lb:0.0000\n",
      "  pred_real_A:0.9858 | pred_fake_A:0.0601\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.7483 | loss_D_A:0.0037 | loss_D_B:0.0000\n",
      "epoch:109/200 batch:667/667 | ra:1.0000 | fa:0.0889 | rb:0.9998 | fb:0.0000 | lg:11.2867 | la:0.0079 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0889\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:11.2867 | loss_D_A:0.0079 | loss_D_B:0.0000\n",
      "epoch:110/200 batch:667/667 | ra:0.9921 | fa:0.0017 | rb:1.0000 | fb:0.0001 | lg:10.7865 | la:0.0001 | lb:0.0000\n",
      "  pred_real_A:0.9921 | pred_fake_A:0.0017\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0001\n",
      "  loss_G:10.7865 | loss_D_A:0.0001 | loss_D_B:0.0000\n",
      "epoch:111/200 batch:667/667 | ra:0.9810 | fa:0.0000 | rb:0.9996 | fb:0.0000 | lg:10.5927 | la:0.0002 | lb:0.0000\n",
      "  pred_real_A:0.9810 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9996 | pred_fake_B:0.0000\n",
      "  loss_G:10.5927 | loss_D_A:0.0002 | loss_D_B:0.0000\n",
      "epoch:112/200 batch:667/667 | ra:0.9883 | fa:0.1374 | rb:0.9999 | fb:0.0050 | lg:11.2050 | la:0.0097 | lb:0.0000\n",
      "  pred_real_A:0.9883 | pred_fake_A:0.1374\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0050\n",
      "  loss_G:11.2050 | loss_D_A:0.0097 | loss_D_B:0.0000\n",
      "epoch:113/200 batch:667/667 | ra:1.0000 | fa:0.0327 | rb:1.0000 | fb:0.0000 | lg:10.4282 | la:0.0008 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0327\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.4282 | loss_D_A:0.0008 | loss_D_B:0.0000\n",
      "epoch:114/200 batch:667/667 | ra:1.0000 | fa:0.1166 | rb:1.0000 | fb:0.0015 | lg:10.4899 | la:0.0136 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.1166\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0015\n",
      "  loss_G:10.4899 | loss_D_A:0.0136 | loss_D_B:0.0000\n",
      "epoch:115/200 batch:667/667 | ra:0.9988 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:12.3458 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9988 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.3458 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:116/200 batch:667/667 | ra:0.9091 | fa:0.0829 | rb:0.9996 | fb:0.0001 | lg:12.6364 | la:0.0151 | lb:0.0000\n",
      "  pred_real_A:0.9091 | pred_fake_A:0.0829\n",
      "  pred_real_B:0.9996 | pred_fake_B:0.0001\n",
      "  loss_G:12.6364 | loss_D_A:0.0151 | loss_D_B:0.0000\n",
      "epoch:117/200 batch:667/667 | ra:1.0000 | fa:0.0042 | rb:1.0000 | fb:0.0000 | lg:10.6128 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0042\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.6128 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:118/200 batch:667/667 | ra:1.0000 | fa:0.0257 | rb:1.0000 | fb:0.0000 | lg:10.8144 | la:0.0005 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0257\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.8144 | loss_D_A:0.0005 | loss_D_B:0.0000\n",
      "epoch:119/200 batch:667/667 | ra:0.8276 | fa:0.0368 | rb:1.0000 | fb:0.0000 | lg:13.8614 | la:0.0301 | lb:0.0000\n",
      "  pred_real_A:0.8276 | pred_fake_A:0.0368\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:13.8614 | loss_D_A:0.0301 | loss_D_B:0.0000\n",
      "epoch:120/200 batch:667/667 | ra:0.9965 | fa:0.0002 | rb:0.9953 | fb:0.0001 | lg:12.8983 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9965 | pred_fake_A:0.0002\n",
      "  pred_real_B:0.9953 | pred_fake_B:0.0001\n",
      "  loss_G:12.8983 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:121/200 batch:667/667 | ra:0.9976 | fa:0.0000 | rb:0.9999 | fb:0.0000 | lg:10.7407 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9976 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:10.7407 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:122/200 batch:667/667 | ra:0.9998 | fa:0.0012 | rb:0.9996 | fb:0.0000 | lg:7.9915 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0012\n",
      "  pred_real_B:0.9996 | pred_fake_B:0.0000\n",
      "  loss_G:7.9915 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:123/200 batch:667/667 | ra:1.0000 | fa:0.0035 | rb:1.0000 | fb:0.0000 | lg:10.0985 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0035\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.0985 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:124/200 batch:667/667 | ra:0.9528 | fa:0.0000 | rb:0.9999 | fb:0.0000 | lg:11.4250 | la:0.0012 | lb:0.0000\n",
      "  pred_real_A:0.9528 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.4250 | loss_D_A:0.0012 | loss_D_B:0.0000\n",
      "epoch:125/200 batch:667/667 | ra:0.9734 | fa:0.0004 | rb:1.0000 | fb:0.0000 | lg:13.3374 | la:0.0007 | lb:0.0000\n",
      "  pred_real_A:0.9734 | pred_fake_A:0.0004\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:13.3374 | loss_D_A:0.0007 | loss_D_B:0.0000\n",
      "epoch:126/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:9.7357 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.7357 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:127/200 batch:667/667 | ra:0.9948 | fa:0.0383 | rb:0.9799 | fb:0.0315 | lg:11.5867 | la:0.0015 | lb:0.0014\n",
      "  pred_real_A:0.9948 | pred_fake_A:0.0383\n",
      "  pred_real_B:0.9799 | pred_fake_B:0.0315\n",
      "  loss_G:11.5867 | loss_D_A:0.0015 | loss_D_B:0.0014\n",
      "epoch:128/200 batch:667/667 | ra:0.9764 | fa:0.1265 | rb:1.0000 | fb:0.0007 | lg:11.8526 | la:0.0094 | lb:0.0000\n",
      "  pred_real_A:0.9764 | pred_fake_A:0.1265\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0007\n",
      "  loss_G:11.8526 | loss_D_A:0.0094 | loss_D_B:0.0000\n",
      "epoch:129/200 batch:667/667 | ra:0.9734 | fa:0.0004 | rb:0.9999 | fb:0.0000 | lg:9.6703 | la:0.0005 | lb:0.00000\n",
      "  pred_real_A:0.9734 | pred_fake_A:0.0004\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:9.6703 | loss_D_A:0.0005 | loss_D_B:0.0000\n",
      "epoch:130/200 batch:667/667 | ra:0.9813 | fa:0.0012 | rb:1.0000 | fb:0.0000 | lg:11.4686 | la:0.0004 | lb:0.0000\n",
      "  pred_real_A:0.9813 | pred_fake_A:0.0012\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.4686 | loss_D_A:0.0004 | loss_D_B:0.0000\n",
      "epoch:131/200 batch:667/667 | ra:1.0000 | fa:0.0004 | rb:1.0000 | fb:0.0001 | lg:11.6961 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0004\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0001\n",
      "  loss_G:11.6961 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:132/200 batch:667/667 | ra:0.9966 | fa:0.3371 | rb:1.0000 | fb:0.0000 | lg:9.4457 | la:0.0796 | lb:0.00000\n",
      "  pred_real_A:0.9966 | pred_fake_A:0.3371\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.4457 | loss_D_A:0.0796 | loss_D_B:0.0000\n",
      "epoch:133/200 batch:667/667 | ra:0.9928 | fa:0.0000 | rb:0.9955 | fb:0.0000 | lg:10.9832 | la:0.0001 | lb:0.0000\n",
      "  pred_real_A:0.9928 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9955 | pred_fake_B:0.0000\n",
      "  loss_G:10.9832 | loss_D_A:0.0001 | loss_D_B:0.0000\n",
      "epoch:134/200 batch:667/667 | ra:0.9716 | fa:0.0287 | rb:1.0000 | fb:0.0000 | lg:12.9559 | la:0.0016 | lb:0.0000\n",
      "  pred_real_A:0.9716 | pred_fake_A:0.0287\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.9559 | loss_D_A:0.0016 | loss_D_B:0.0000\n",
      "epoch:135/200 batch:667/667 | ra:0.9730 | fa:0.0031 | rb:0.9998 | fb:0.0000 | lg:13.1930 | la:0.0007 | lb:0.0000\n",
      "  pred_real_A:0.9730 | pred_fake_A:0.0031\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:13.1930 | loss_D_A:0.0007 | loss_D_B:0.0000\n",
      "epoch:136/200 batch:667/667 | ra:0.9999 | fa:0.0015 | rb:0.9998 | fb:0.0000 | lg:9.8395 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0015\n",
      "  pred_real_B:0.9998 | pred_fake_B:0.0000\n",
      "  loss_G:9.8395 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:137/200 batch:667/667 | ra:0.9691 | fa:0.0006 | rb:0.9999 | fb:0.0000 | lg:13.2760 | la:0.0010 | lb:0.0000\n",
      "  pred_real_A:0.9691 | pred_fake_A:0.0006\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:13.2760 | loss_D_A:0.0010 | loss_D_B:0.0000\n",
      "epoch:138/200 batch:667/667 | ra:0.9997 | fa:0.0000 | rb:1.0000 | fb:0.0062 | lg:9.0060 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9997 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0062\n",
      "  loss_G:9.0060 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:139/200 batch:667/667 | ra:0.9972 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.7256 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9972 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.7256 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:140/200 batch:667/667 | ra:0.9922 | fa:0.4876 | rb:1.0000 | fb:0.0000 | lg:8.3167 | la:0.1488 | lb:0.00000\n",
      "  pred_real_A:0.9922 | pred_fake_A:0.4876\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.3167 | loss_D_A:0.1488 | loss_D_B:0.0000\n",
      "epoch:141/200 batch:667/667 | ra:0.9994 | fa:0.0290 | rb:1.0000 | fb:0.0129 | lg:12.1522 | la:0.0008 | lb:0.0002\n",
      "  pred_real_A:0.9994 | pred_fake_A:0.0290\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0129\n",
      "  loss_G:12.1522 | loss_D_A:0.0008 | loss_D_B:0.0002\n",
      "epoch:142/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:9.4090 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.4090 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:143/200 batch:667/667 | ra:0.9936 | fa:0.0000 | rb:0.9999 | fb:0.0014 | lg:11.6960 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9936 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0014\n",
      "  loss_G:11.6960 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:144/200 batch:667/667 | ra:0.9567 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:8.6589 | la:0.0019 | lb:0.00000\n",
      "  pred_real_A:0.9567 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.6589 | loss_D_A:0.0019 | loss_D_B:0.0000\n",
      "epoch:145/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0001 | lg:12.9374 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0001\n",
      "  loss_G:12.9374 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:146/200 batch:667/667 | ra:0.9707 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:13.6836 | la:0.0005 | lb:0.0000\n",
      "  pred_real_A:0.9707 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:13.6836 | loss_D_A:0.0005 | loss_D_B:0.0000\n",
      "epoch:147/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:7.8640 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:7.8640 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:148/200 batch:667/667 | ra:0.9088 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.8484 | la:0.0081 | lb:0.0000\n",
      "  pred_real_A:0.9088 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.8484 | loss_D_A:0.0081 | loss_D_B:0.0000\n",
      "epoch:149/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:8.7726 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.7726 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:150/200 batch:667/667 | ra:1.0000 | fa:0.0028 | rb:1.0000 | fb:0.0000 | lg:12.5310 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0028\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.5310 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:151/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:9.3914 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.3914 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:152/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:0.9999 | fb:0.0000 | lg:11.1974 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.1974 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:153/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:12.8275 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.8275 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:154/200 batch:667/667 | ra:0.9995 | fa:0.0177 | rb:0.9999 | fb:0.0008 | lg:10.3818 | la:0.0003 | lb:0.0000\n",
      "  pred_real_A:0.9995 | pred_fake_A:0.0177\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0008\n",
      "  loss_G:10.3818 | loss_D_A:0.0003 | loss_D_B:0.0000\n",
      "epoch:155/200 batch:667/667 | ra:0.9999 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:12.1995 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.1995 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:156/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:8.3179 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.3179 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:157/200 batch:667/667 | ra:0.9992 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:12.8864 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9992 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.8864 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:158/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:7.7094 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:7.7094 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:159/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:9.4930 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.4930 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:160/200 batch:667/667 | ra:0.9999 | fa:0.0009 | rb:1.0000 | fb:0.0000 | lg:12.3827 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0009\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.3827 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:161/200 batch:667/667 | ra:0.9975 | fa:0.0010 | rb:1.0000 | fb:0.0000 | lg:9.5933 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9975 | pred_fake_A:0.0010\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.5933 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:162/200 batch:667/667 | ra:0.9982 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.6110 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9982 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.6110 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:163/200 batch:667/667 | ra:0.9834 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.5360 | la:0.0003 | lb:0.0000\n",
      "  pred_real_A:0.9834 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.5360 | loss_D_A:0.0003 | loss_D_B:0.0000\n",
      "epoch:164/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:10.4674 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.4674 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:165/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:0.9988 | fb:0.0000 | lg:10.6146 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:0.9988 | pred_fake_B:0.0000\n",
      "  loss_G:10.6146 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:166/200 batch:667/667 | ra:0.9999 | fa:0.0029 | rb:1.0000 | fb:0.0000 | lg:9.4359 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0029\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.4359 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:167/200 batch:667/667 | ra:0.9984 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.5649 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9984 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.5649 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:168/200 batch:667/667 | ra:0.9999 | fa:0.0017 | rb:0.9999 | fb:0.0000 | lg:11.3504 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0017\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.3504 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:169/200 batch:667/667 | ra:0.9984 | fa:0.0315 | rb:1.0000 | fb:0.0000 | lg:10.5533 | la:0.0010 | lb:0.0000\n",
      "  pred_real_A:0.9984 | pred_fake_A:0.0315\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.5533 | loss_D_A:0.0010 | loss_D_B:0.0000\n",
      "epoch:170/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:11.2447 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.2447 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:171/200 batch:667/667 | ra:0.9998 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:10.7576 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.7576 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:172/200 batch:667/667 | ra:0.9999 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:12.1937 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.1937 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:173/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0024 | lg:12.1689 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0024\n",
      "  loss_G:12.1689 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:174/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:10.6695 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.6695 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:175/200 batch:667/667 | ra:0.9999 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.4431 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.4431 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:176/200 batch:667/667 | ra:0.9996 | fa:0.0009 | rb:1.0000 | fb:0.0000 | lg:9.2838 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:0.9996 | pred_fake_A:0.0009\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.2838 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:177/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:9.8623 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.8623 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:178/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:0.9997 | fb:0.0000 | lg:11.4154 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9997 | pred_fake_B:0.0000\n",
      "  loss_G:11.4154 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:179/200 batch:667/667 | ra:0.9996 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:10.8464 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9996 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.8464 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:180/200 batch:667/667 | ra:1.0000 | fa:0.0002 | rb:1.0000 | fb:0.0000 | lg:9.5931 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0002\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.5931 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:181/200 batch:667/667 | ra:1.0000 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:9.2732 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.2732 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:182/200 batch:667/667 | ra:0.9998 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:11.0053 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.0053 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:183/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:11.8179 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.8179 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:184/200 batch:667/667 | ra:0.9972 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:11.0306 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9972 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.0306 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:185/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.2662 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.2662 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:186/200 batch:667/667 | ra:1.0000 | fa:0.0003 | rb:0.9999 | fb:0.0000 | lg:11.6173 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0003\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.6173 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:187/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:11.8892 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.8892 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:188/200 batch:667/667 | ra:0.9879 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:16.2037 | la:0.0001 | lb:0.0000\n",
      "  pred_real_A:0.9879 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:16.2037 | loss_D_A:0.0001 | loss_D_B:0.0000\n",
      "epoch:189/200 batch:667/667 | ra:0.9998 | fa:0.0000 | rb:1.0000 | fb:0.0003 | lg:11.4904 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0003\n",
      "  loss_G:11.4904 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:190/200 batch:667/667 | ra:0.9993 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:13.3403 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9993 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:13.3403 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:191/200 batch:667/667 | ra:0.9998 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:11.0546 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9998 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:11.0546 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:192/200 batch:667/667 | ra:0.9999 | fa:0.0000 | rb:0.9999 | fb:0.0000 | lg:11.8986 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0000\n",
      "  pred_real_B:0.9999 | pred_fake_B:0.0000\n",
      "  loss_G:11.8986 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:193/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:8.7966 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:8.7966 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:194/200 batch:667/667 | ra:1.0000 | fa:0.0011 | rb:1.0000 | fb:0.0000 | lg:9.8270 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0011\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.8270 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:195/200 batch:667/667 | ra:0.9999 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.3005 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.3005 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:196/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:7.5845 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:7.5845 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:197/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:10.7126 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:10.7126 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:198/200 batch:667/667 | ra:0.9999 | fa:0.0001 | rb:1.0000 | fb:0.0000 | lg:12.6777 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9999 | pred_fake_A:0.0001\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:12.6777 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:199/200 batch:667/667 | ra:1.0000 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:9.3216 | la:0.0000 | lb:0.00000\n",
      "  pred_real_A:1.0000 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:9.3216 | loss_D_A:0.0000 | loss_D_B:0.0000\n",
      "epoch:200/200 batch:667/667 | ra:0.9994 | fa:0.0000 | rb:1.0000 | fb:0.0000 | lg:13.6641 | la:0.0000 | lb:0.0000\n",
      "  pred_real_A:0.9994 | pred_fake_A:0.0000\n",
      "  pred_real_B:1.0000 | pred_fake_B:0.0000\n",
      "  loss_G:13.6641 | loss_D_A:0.0000 | loss_D_B:0.0000\n"
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss (lambda=5)\n",
    "        # G_A2B(B) should equal B if real B is fed     \n",
    "        loss_identity_A = criterion_identity(netG_B2A(real_A), real_A) * 5.0\n",
    "\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        loss_identity_B = criterion_identity(netG_A2B(real_B), real_B) * 5.0\n",
    "\n",
    "        # GAN loss (lambda=1)\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_A = netD_A(fake_A)\n",
    "        pred_B = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_B, target_real)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_A, target_real)\n",
    "\n",
    "        # Cycle loss (lambda=10)\n",
    "        fake_ABA = netG_B2A(fake_B)\n",
    "        fake_BAB = netG_A2B(fake_A)\n",
    "        loss_cycle_ABA = criterion_cycle(fake_ABA, real_A) * 10.0\n",
    "        loss_cycle_BAB = criterion_cycle(fake_BAB, real_B) * 10.0\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_A = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real_A, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake_A = netD_A(fake_A)\n",
    "        loss_D_fake = criterion_GAN(pred_fake_A, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_B = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real_B, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake_B = netD_B(fake_B)\n",
    "        loss_D_fake = criterion_GAN(pred_fake_B, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "        print(\n",
    "            f'\\repoch:{epoch+1}/{opt.n_epochs} batch:{i + 1}/{len(dataloader)}'\n",
    "            + f' | ra:{pred_real_A.mean():.4f}'\n",
    "            + f' | fa:{pred_fake_A.mean():.4f}'\n",
    "            + f' | rb:{pred_real_B.mean():.4f}'\n",
    "            + f' | fb:{pred_fake_B.mean():.4f}'\n",
    "            + f' | lg:{loss_G:.4f}'\n",
    "            + f' | la:{loss_D_A:.4f}'\n",
    "            + f' | lb:{loss_D_B:.4f}',\n",
    "            end='',\n",
    "        )\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'epoch:{epoch+1}/{opt.n_epochs}')\n",
    "    print(f'  pred_real_A:{pred_real_A.mean():.4f} | pred_fake_A:{pred_fake_A.mean():.4f}')\n",
    "    print(f'  pred_real_B:{pred_real_B.mean():.4f} | pred_fake_B:{pred_fake_B.mean():.4f}')\n",
    "    print(f'  loss_G:{loss_G:.4f} | loss_D_A:{loss_D_A:.4f} | loss_D_B:{loss_D_B:.4f}')\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Testing######\n",
    "\n",
    "# Set model's test mode\n",
    "netG_A2B.eval()\n",
    "netG_B2A.eval()\n",
    "\n",
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [ transforms.Resize(int(opt.size), Image.BICUBIC), transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, mode='test'), \n",
    "                        batch_size=opt.batchSize, shuffle=False, num_workers=opt.n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images 0070 of 0070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create output dirs if they don't exist\n",
    "if not os.path.exists('output/A'):\n",
    "    os.makedirs('output/A')\n",
    "if not os.path.exists('output/B'):\n",
    "    os.makedirs('output/B')\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # Set model input\n",
    "    real_A = Variable(input_A.copy_(batch['A']))\n",
    "    real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "    # Generate output\n",
    "    fake_B = 0.5*(netG_A2B(real_A).data + 1.0)\n",
    "    fake_A = 0.5*(netG_B2A(real_B).data + 1.0)\n",
    "\n",
    "    # Save image files\n",
    "    save_image(fake_A, 'output/A/%04d.png' % (i+1))\n",
    "    save_image(fake_B, 'output/B/%04d.png' % (i+1))\n",
    "\n",
    "    sys.stdout.write('\\rGenerated images %04d of %04d' % (i+1, len(dataloader)))\n",
    "\n",
    "sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
